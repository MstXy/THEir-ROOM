# THEir-ROOM
 Fall 2021 A.rt I.ntel Project 2: Visual

 Jannie Zhou [yz4970@nyu.edu](mailto:yz4970@nyu.edu)		William Zhang [cz1627@nyu.edu](mailto:cz1627@nyu.edu)

### Project Link: https://mstxy.github.io/THEir-ROOM/

### ABSTRACT
For the visual project, we utilized several machine learning algorithms to narrate an imagined dystopian future of human development. By presenting our content on the website in an exhibition manner, we hope it can provoke certain discussions and thoughts on the future of artificial intelligence and pose some critical questions towards how we should move forward with the development of artificial intelligence.

### 1.	INSPIRATION
After experimenting with a few machine learning algorithms concerning images, we are intrigued by the images of daily objects generated by the BigGAN algorithm. Some of the images look very realistic while others being completely distorted and unreal. The inaccuracy reflects the data that the algorithm is trained on and the complexity of the task (e.g. we found that all the pictures containing humans are very distorted, especially human faces while pictures of animals are more accurate).
After reading The AI Revolution: The Road to Superintelligence, we are intrigued by the concept of artificial superintelligence (ASI) brought up by Urban. Oxford philosopher and leading AI thinker Nick Bostrom define superintelligence as “an intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills”. Urban keeps emphasizing we are on edge of the exponential growth of artificial intelligence and proposes several approaches to creating ASI. The article got us thinking about what would happen if ASI is created and how will the inaccuracy and bias of the current machine learning algorithm influence the greater society and human history.
Based on the Speculative Design we did a few weeks ago, we decided to use images to tell a story of an imagined dystopian future where humanity has gone extinct and the history of humanity is represented by these "inaccurate and biased" images generated by artificial intelligence. We are also heavily inspired by the episode of Three Robots from Love Death + Robots. So we set the subject of our story as aliens and tell the story from their perspective to create irony.

### 2.	RESEARCH
We did some research into the main machine learning algorithm we are going to use: BigBAN, and found out a lot of people are using BigBAN as a creative engine to "metamorphosis everything" (Alex). In our era where there is so much data handy, artificial intelligence became a new innovative way to produce artworks. This phoemoneon also got us question the definition of creativity and what is the difference between human creativity and artificial intelligence creativity. If those two could be considered equivalent, what kind of future awaits for human artworks and history in general? Our answer to this question is artificial intelligence creativity should not be equated with human creativity, no matter how much machine learning algorithms are trying to mimic our neural network. Hence we want to spark some disscussion and raise awareness of this topic by presenting a dystopian future where artificial intelligence has taken over and reflect humanity history in a distorted way. We must be very carefully when we are developing A.I., technology and ethics should develop together when something new came along.

### 3.	PRODUCTION
#### 3.1	Project Title & Idea
Our project is named THEir Room. We give it three layers of meaning: 1) Our project is about a reconstructed human room, made by aliens using a database found on post-apocalypse earth. So the room is “their room” according to which aliens tell about human’s stories. 2) Because it is actually a reconstructed room using A.I of irregular quality, the room is also “The Irregular Room”, full of irrationality of A.I. 3) While A.I is in main control of creating this room, and aliens choose to full believe in it, it is “The Room”, the jailed room of the shaped reality. The aliens in our project could well be us. Aliens choose to fully believe because they have no context or background knowledge about humans. Thus, it urges us to reflect whether in reality, when we use A.I, are we actually these aliens, neglecting other contextual information and blindly trust in the “room” A.I created for us?

#### 3.2	Background Storytelling
In order to inform our audience about the background of our website, we decided to make a video. In this video, we sampled some clips from different video sources including YouTube Channel Maverick and S01 E02 of Love Death + Robots. The detailed reference is attached in section 5. After the video, we added some text to further explain the background; And because in this project we deliberately choose not to mention the A.I participation in this story, we added some hint, naming the database found that aliens use to tell the story of humans: BigGAN-GPT2.

#### 3.3	Main Page Exploration
Our project’s main page is the reconstructed human room. We use BigGAN and its community [Google Colab implementation](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb) to generate 106 objects and choose 16 among them to create our fictional room. On hovering each object image, the website will present a short object description in a somehow subjective tone. This is achieved by using the GPT-2 implementation in [InferKit](https://app.inferkit.com/demo), where by inputting “This is *object name*”, we let the algorithms auto-complete the paragraph, with some minor manual tweaking on the sentences. The descriptions themselves already have a striking impact as to how real and subjective it seems to be. We want to further promote such an impact on our users. So we use [Online Microsoft Sam TTS Generator](https://tetyys.com/SAPI4/) to generate text-to-speech for every line of object description. The sound is robotic and monotonous, but combined with the subjective sentences, the voices somehow also come to life.

#### 3.4	Further Object Exploration
To make the information on this website more realistic (for the aliens), we created pseudo-3D animation for the objects’ images using [Colab notebook](https://colab.research.google.com/drive/1hxx4iSuAOyeI2gCL54vQkpEuBVrIv1hY) for 3D Ken Burns Effect. Originally, we planned to do a 3D effect for our constructed room, but the effect does not come out as ideal as what we would’ve expected, probably due to the flatness of the scene. So we decided to switch to making individual 3D effects for the objects. On mouse hovering, the images of each object will start to present 3D visual effects. Here, we also include object descriptions, which is also achieved using GPT-2, but using different tools. We want to differentiate the text here with that of the previous page. The text here is more informative, and is presented from the perspective of aliens. We combine the InferKit implementation (which is actually named Talk to Transformer) with [Write With Transformer](https://transformer.huggingface.co/doc/gpt2-large), to present an objective view towards these A.I generated objects. We further make some tweaking to the sentences, to point out some of the inconsistencies in A.I generated objects from the perspective of ignorant aliens. For example: on pointing out the 7 to 8 hands on a clock, aliens comment: “we see how complicated these clocks must be in their usage during human times”; and on how beer glass inherently have beer in it, aliens comment: “It is amazing how these beerglasses appear that there are already liquid in it. Really fine craftskill from humans”; and commenting on the horrible shape of A.I generated teddy bear: “Its shape though, really concerns the researchers: malformed and strange.” There are much more to explore, and each alien comment is really a comment on the bias and inconsistencies on A.I.

#### 3.5	Supplementary Storytelling
In the last page “Our Goal”, we present this website from the alien perspective, emphasizing the realness of the constructed room. We didn’t point out explicitly that A.I is the main participant in the reconstruction, because 1) our aliens in the story don’t actually know 2) it pushes our viewers to reflect on the caveat of neglecting A.I’s limited ability. To incorporate more creative A.I tools and utilize them in our storytelling, we use the [Colab notebook](https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP_%28z%2Bquantize_method_with_augmentations%2C_user_friendly_interface%29.ipynb) of VQGAN-CLIP trained on Wikiart to help visualize some lines of story. We input our prompt and use whatever that comes out of the algorithm. It is arguable that this is also shaping the “room”, as before seeing the images, users may have their own interpretation and imagination of the texts.

### 4.	FUTURE DEVELOPMENT
#### 4.1	3D Ken Burns Effect
The 3D Ken Burns Effect we used in our object pictures did not perform well as it does in other pictures with more details and layers. We tried to apply this effect to our collage, but the algorithms failed to treat this picture as an integrated part. So if we want to try to enhance the 3D effect, for example refining our pictures to have more layers.
#### 4.2	Text & Sound
There is a relative high portion of text and sound in our website, which we deliberately put there to enhance the sense of ridiculousness and irony. However, as the course continues we are going to study more machine learning algorithms about sounds and texts. Hence if we have the opportunity to improve our texts and sound, we would love to come back to this project.

### 5.	REFERENCES
[1]	Urban, Tim. “The AI Revolution: The Road to Superintelligence.” Wait But Why, 7 Sept. 2017, waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html.
[2]	Brock, Andrew et al.. “Large Scale GAN Training for High Fidelity Natural Image Synthesis.” ICLR 2019, 25 Feb. 2019, https://arxiv.org/abs/1809.11096.
[3]	“Better Language Models and Their Implications (GPT-2).” OpenAI, https://openai.com/blog/better-language-models/.
[4]	Niklaus, Simon et al.. “3D Ken Burns Effect from a Single Image.”  CVPR 2019, 12 Sept. 2019, https://arxiv.org/abs/1909.05483.
[5]	“Quarantine - Post Apocalyptic Scene - Unreal Engine 4.” YouTube, uploaded by Maverick, 6 Apr. 2020, www.youtube.com/watch?v=uth3OjRkt0s&feature=youtu.be.
[6]	Miller, Tim, et al. “Three Robots.” Netflix, uploaded by Netflix, 15 Mar. 2019, www.netflix.com/ae-en/title/80174608.
